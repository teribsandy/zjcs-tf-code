'''kaggleæ•°æ®titanicç”Ÿå­˜é¢„æµ‹
æ–¹æ³•ï¼štensorflowçš„ã€kerasã€‘ å®ç° å¯¹æ•°å‡ ç‡å›å½’ï¼ˆé€»è¾‘å›å½’ï¼‰'''

import numpy as np
import pandas as pd

'''è®­ç»ƒé›†è¯»æ•°æ®+é¢„å¤„ç†'''
df=pd.read_csv('/Users/zhangying/Desktop/tf-exercise/titanic/train.csv') #(891,12)
data=df[['Survived','Pclass','Sex','Age','SibSp','Parch','Fare','Cabin','Embarked']]
data['Age'].fillna(data['Age'].mean(),inplace=True)
data['Cabin']=pd.factorize(data.Cabin)[0]
data['Sex']=[1 if x=='male' else 0 for x in data.Sex]
data['p1']=np.array(data['Pclass']==1).astype(np.int32)
data['p2']=np.array(data['Pclass']==2).astype(np.int32)
data['p3']=np.array(data['Pclass']==3).astype(np.int32)
del data['Pclass']
data['e1']=np.array(data['Embarked']=='S').astype(np.int32)
data['e2']=np.array(data['Embarked']=='C').astype(np.int32)
data['e3']=np.array(data['Embarked']=='Q').astype(np.int32)
del data['Embarked']

train_data=data[['Sex','Age','SibSp','Parch','Fare','Cabin','p1','p2','p3','e1','e2','e3']] #(891,12)
#      Sex        Age  SibSp  Parch      Fare  Cabin  p1  p2  p3  e1  e2  e3
# 0      1  22.000000      1      0    7.2500     -1   0   0   1   1   0   0
# 1      0  38.000000      1      0   71.2833      0   1   0   0   0   1   0
# 2      0  26.000000      0      0    7.9250     -1   0   0   1   1   0   0
# 3      0  35.000000      1      0   53.1000      1   1   0   0   1   0   0
train_target=data[['Survived']] #(891,1)



'''kerasæ­å»ºç¥ç»ç½‘ç»œ'''
from tensorflow import keras
from tensorflow.keras import layers
model=keras.Sequential()
model.add(layers.Dense(1,input_dim=12,activation='sigmoid'))
# model.summary()
model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['acc'])
hist=model.fit(train_data,train_target,epochs=300)
#print(hist.history) #å­—å…¸ ï¼ˆä¸å¦¨é€šè¿‡hist.history.keys()æŸ¥çœ‹æœ‰å“ªäº›é”® dict_keys(['loss','acc])      ï¼‰

import matplotlib.pyplot as plt
plt.plot(range(300),hist.history.get('loss'))
plt.show()


'''tensorflowæ­å»ºç¥ç»ç½‘ç»œ'''

# import tensorflow as tf
# x=tf.placeholder(tf.float32,shape=[None,12])
# y=tf.placeholder(tf.float32,shape=[None,1])

# weight=tf.Variable(tf.random_normal([12,1]))
# bias=tf.Variable(tf.random_normal([1]))
# output=tf.matmul(x,weight)+bias

# loss=tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y,logits=output))
# pred=tf.cast(tf.sigmoid(output)>0.5,tf.float32)
# accuracy=tf.reduce_mean(tf.cast(tf.equal(pred,y),tf.float32))

# train_step=tf.train.GradientDescentOptimizer(0.003).minimize(loss)

# sess=tf.Session()
# sess.run(tf.global_variables_initializer())

# epoch=25000
# train_loss=[]
# train_accuracy=[]
# test_accuracy=[]
# for i in range(epoch):
#     #æ‰“ä¹±æ ·æœ¬æ•°æ®çš„é¡ºåº
#     #df=df.sample(frac=1) è¿™ç§æ–¹æ³•æ¯”è¾ƒç®€ä¾¿ï¼ä½†æ˜¯train_dataçš„æ‰“ä¹±é¡ºåºè¦å’Œtrain_targetä¿æŒä¸€è‡´ï¼Œæ‰€ä»¥é€‰ğŸ‘‡çš„æ–¹æ³•
#     shuffledindex=np.random.permutation(len(train_data))
#     train_data=train_data.iloc[shuffledindex]
#     train_target=train_target.iloc[shuffledindex]
    
#     #batchsize=100,å°æ‰¹é‡æ¢¯åº¦ä¸‹é™ï¼Œ9ä¸ªæ‰¹æ¬¡æ‰€ä»¥æ›´æ–°9æ¬¡å‚æ•°
#     for n in range(len(train_data)//100 +1):
#         batch_train_data=train_data[100*n:100*(n+1)]
#         batch_train_target=train_target[100*n:100*(n+1)]
#         sess.run(train_step,feed_dict={x:batch_train_data,y:batch_train_target})
    
#     #æ¯å®Œæˆ1000æ¬¡epochåï¼Œè®°å½•æŸå¤±å‡½æ•°å’Œaccuracy
#     #loss_tempå’Œtrain_accuracy_tempç”¨çš„æ˜¯batchæ•°æ®è·‘èµ·æ¥éƒ½å¥½æ…¢ï¼ŒèŠ±äº†å·®ä¸å¤š10åˆ†é’Ÿ....ğŸ˜¢
#     #å…¶å®batchsizeåº”è¯¥æ˜¯è¶Šå¤§è¶Šå¥½çš„ï¼Œå¯æ˜¯å°ç©·é¬¼çš„ç¡¬ä»¶æ°´å¹³æœ‰é™...æ‰€ä»¥batchsizeä¸º100ã€200
#     if i%1000==0:
#         loss_temp=sess.run(loss,feed_dict={x:batch_train_data,y:batch_train_target})
#         train_loss.append(loss_temp)

#         train_accuracy_temp=sess.run(accuracy,feed_dict={x:batch_train_data,y:batch_train_target})
#         train_accuracy.append(train_accuracy_temp)

#         test_accuracy_temp=sess.run(accuracy,feed_dict={x:test_data,y:test_target})
#         test_accuracy.append(test_accuracy_temp)

#         print (loss_temp,train_accuracy_temp,test_accuracy_temp)
# sess.close()

